{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 0.1 Accelerating AI Inference: A Deep Dive into NVIDIA Dynamo on a K8s Cluster\n",
    "Welcome to Accelerating AI Inference: A Deep Dive into NVIDIA Dynamo on a Kubernetes Cluster!<br>\n",
    "\n",
    "In this course you'll learn how to deploy, configure, and optimize NVIDIA Dynamo, a high-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments. This course will demonstrate the complete lifecycle of Dynamo deployment on Kubernetes, from basic setup to advanced disaggregated serving and performance optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0.1.1 What is NVIDIA Dynamo?\n",
    "\n",
    "NVIDIA Dynamo is a high-throughput, low-latency inference framework specifically designed for serving generative AI and reasoning models in multi-node distributed environments. It provides several key innovations:\n",
    "\n",
    "### Key Features:\n",
    "- **Disaggregated prefill & decode inference**: Maximizes GPU throughput and helps balance throughput and latency\n",
    "- **Dynamic GPU scheduling**: Optimizes performance based on real-time demand  \n",
    "- **LLM-aware request routing**: Eliminates unnecessary KV cache recomputation\n",
    "- **Accelerated data transfer**: Reduces inference response time through NVIDIA Inference Transfer Library (NIXL)\n",
    "- **Inference engine agnostic**: Supports TRT-LLM, vLLM, SGLang and others\n",
    "\n",
    "### Architecture Benefits:\n",
    "- **40% improvement in TTFT (Time To First Token)** with KV cache system memory offloading\n",
    "- **Efficient multi-node scaling** with disaggregated serving architecture\n",
    "- **Reduced latency** through intelligent request routing and caching\n",
    "- **Resource optimization** via dynamic GPU scheduling and workload distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# NVIDIA Dynamo Architecture & Concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2 High-Level Architecture\n",
    "\n",
    "NVIDIA Dynamo is designed around several key architectural principles that address the unique challenges of serving large language models (LLMs) and generative AI workloads at scale.\n",
    "\n",
    "### Core Design Principles:\n",
    "\n",
    "1. **Inference Engine Agnostic**: Supports multiple backends including TensorRT-LLM, vLLM, and SGLang\n",
    "2. **Distributed by Design**: Built for multi-node, multi-GPU environments from the ground up\n",
    "3. **Performance Optimized**: Focus on both throughput and latency optimization\n",
    "4. **Resource Efficient**: Smart resource allocation and utilization\n",
    "\n",
    "\n",
    "The following diagrams illustrate the architecture of the Dynamo platform:\n",
    "\n",
    "<center><img src=\"images/dynamo/dynamo_architecture.png\"></center>\n",
    "\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Dynamo Planner**: Intelligent workload distribution and GPU scheduling\n",
    "- **KV Block Manager**: Efficient key-value cache management across memory tiers\n",
    "- **Request Router**: LLM-aware routing to minimize recomputation\n",
    "- **NIXL (NVIDIA Inference Transfer Library)**: Accelerated data transfer\n",
    "- **Disaggregated Serving**: Separate prefill and decode operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.3 Disaggregated Serving\n",
    "\n",
    "One of Dynamo's most powerful features is its ability to disaggregate the inference process into separate **prefill** and **decode** operations. This architectural choice provides significant performance benefits.\n",
    "\n",
    "### Traditional vs. Disaggregated Serving\n",
    "\n",
    "**Traditional Approach:**\n",
    "The prefill phase processes user input to generate the first output token and is compute-bound, while the decode phase generates subsequent tokens and is memory-bound. Co-locating these phases on the same GPU or GPU node leads to inefficient resource use, especially for long input sequences. Additionally, the distinct hardware needs of each phase limit model parallelism flexibility, causing missed performance opportunities.\n",
    "\n",
    "- Single GPU/node handles both prefill (processing input prompt) and decode (generating tokens)\n",
    "- Resource contention between different phases of inference\n",
    "- Suboptimal GPU utilization patterns\n",
    "\n",
    "**Dynamo's Disaggregated Approach:**\n",
    "To address these issues, disaggregated serving separates the prefill and decode phases onto different GPUs or nodes. This enables developers to optimize each phase independently, applying different model parallelism strategies and assigning different GPU devices to each phase\n",
    "- **Prefill nodes**: Optimized for high-throughput batch processing of input prompts\n",
    "- **Decode nodes**: Optimized for low-latency token generation\n",
    "- Independent scaling of each phase based on workload characteristics\n",
    "\n",
    "<center><img src=\"images/dynamo/disaggregated-serving-traditional-serving-comparison-2.png\" width=\"800\" alt=\"Disaggregated Serving Architecture\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.4 KV Cache Management\n",
    "\n",
    "The Key-Value (KV) cache is critical for LLM performance, storing computed attention states to avoid recomputation. Dynamo introduces sophisticated KV cache management that spans multiple memory tiers.\n",
    "\n",
    "### KV Block Manager (KVBM) Architecture\n",
    "\n",
    "\n",
    "The KVBM serves as a critical infrastructure component for scaling LLM inference workloads efficiently. By cleanly separating runtime logic from memory management, and by enabling distributed block sharing, KVBM lays the foundation for high-throughput, multi-node, and memory-disaggregated AI systems.\n",
    "\n",
    "<center><img src=\"images/dynamo/kvbm-arch.png\" width=\"700\" alt=\"KV Block Manager Architecture\"></center>\n",
    "\n",
    "The KVBM has three primary logical layers. The top layer-the LLM inference runtimes (TRTLLM, vLLM and SGLang)-integrates through a dedicated connector module to the Dynamo KVBM module. These connectors act as translation layers, mapping runtime-specific operations and events into the KVBM’s block-oriented memory interface. This decouples memory management from the inference runtime, enabling backend portability and providing memory tiering.\n",
    "\n",
    "The middle layer, the KVBM layer, encapsulates the core logic of the KV block manager and serves as the runtime substrate for managing block memory. The KVBM adapter layer normalizes the representations and data layout for the incoming requests across runtimes and forwards them to the core memory manager. The KVBM and the core modules implement required internal functionality, such as table lookups, memory allocation, block layout management, lifecycle, and state transitions and block reuse or eviction was on policies. The KVBM layer also has required abstractions for external components to override or augment its behavior.\n",
    "\n",
    "The last layer, the NIXL layer, provides unified support for enabling all data and storage transactions. NIXL enables P2P GPU transfers, enables RDMA and NVLINK remote memory sharing, dynamic block registration and metadata exchange and provides a plugin interface for storage backends.\n",
    "\n",
    "NIXL integrates with several backends:\n",
    "\n",
    "- Block memory (Eg. GPU HBM, Host DRAM, Remote DRAM, Local SSD when exposed as block device)\n",
    "- Local file system (for example, POSIX)\n",
    "- Remote file system (for example, NFS)\n",
    "- Object stores (for example, S3-compatible)\n",
    "- Cloud storage (for example, blob storage APIs)\n",
    "\n",
    "More reading here: https://docs.nvidia.com/dynamo/latest/architecture/kvbm_architecture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.5 LLM-Aware Request Routing\n",
    "\n",
    "Dynamo's request router is designed specifically for LLM workloads, understanding the unique characteristics of language model inference to optimize routing decisions.\n",
    "\n",
    "### Traditional vs. LLM-Aware Routing\n",
    "\n",
    "**Traditional Load Balancing:**\n",
    "- Round-robin or random distribution\n",
    "- No awareness of model state or cache content\n",
    "- Higher cache misses and recomputation\n",
    "\n",
    "**Dynamo's LLM-Aware Routing:**\n",
    "- Routes requests to nodes with relevant KV cache data\n",
    "- Minimizes cache misses and recomputation\n",
    "- Optimizes for conversation continuity and prefix matching\n",
    "\n",
    "### Routing Strategies:\n",
    "\n",
    "1. **Prefix-Aware Routing**: Routes requests with common prefixes to the same node\n",
    "2. **Cache-Aware Routing**: Considers existing KV cache when making routing decisions\n",
    "3. **Load-Balanced Routing**: Balances load while maintaining cache efficiency\n",
    "4. **Conversation Affinity**: Keeps multi-turn conversations on the same node when beneficial\n",
    "\n",
    "### Performance Impact:\n",
    "\n",
    "- **Reduced latency** through cache hit optimization\n",
    "- **Lower compute requirements** due to reduced recomputation\n",
    "- **Better resource utilization** across the cluster\n",
    "- **Improved user experience** with faster response times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.6 Dynamic GPU Scheduling and Planning\n",
    "\n",
    "The Dynamo Planner is responsible for intelligent workload distribution and resource allocation across the cluster. It provides both load-based and SLA-based planning strategies. The planner monitors the state of the system and adjusts workers to ensure that the system runs efficiently.\n",
    "\n",
    "Currently, the planner can scale the number of vllm workers up and down based on the kv cache load and prefill queue size. Key features include:\n",
    "- Load-based scaling that monitors KV cache utilization and prefill queue size to make scaling decisions\n",
    "- SLA-based scaling that uses predictive modeling and performance interpolation to proactively meet TTFT and ITL targets\n",
    "- Multi-backend support for both local (Circus) and Kubernetes environments\n",
    "- Graceful scaling that ensures no requests are dropped during scale-down operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.7 NVIDIA Inference Transfer Library (NIXL)\n",
    "\n",
    "Large-scale distributed inference leverages model parallelism techniques such as Tensor, pipeline, and expert parallelism, which rely on internode and intranode, low-latency, high-throughput communication leveraging GPUDirect RDMA. These systems also require rapid KV cache transfer between prefill and decode GPU workers in disaggregated serving environments. \n",
    "\n",
    "Additionally, they must support accelerated communication libraries that are both hardware- and network-agnostic, capable of efficiently moving data across GPUs and memory hierarchies including storage—like CPU memory, and block, file, and object storage—and compatible with a range of networking protocols.\n",
    "\n",
    "\n",
    "<center><img src=\"images/dynamo/nvidia-inference-transfer-library.png\" width=\"700\" alt=\"KV Block Manager Architecture\"></center>\n",
    "\n",
    "\n",
    "NVIDIA Inference Transfer Library (NIXL) is a high-throughput, low-latency point-to-point communication library that provides a consistent data movement API to move data rapidly and asynchronously across different tiers of memory and storage using the same semantics. It is specifically optimized for inference data movement, supporting nonblocking and noncontiguous data transfers between various types of memory and storage. \n",
    "\n",
    "NIXL supports heterogeneous data paths as well as different types of memory and local SSDs, plus networked storage from key NVIDIA storage partners.  \n",
    "\n",
    "NIXL enables NVIDIA Dynamo to interface with other communications libraries such as GPUDirect Storage, UCX, and S3 with a common API regardless of whether the transfer is over  NVLink (C2C or NVSwitch), InfiniBand, RoCE, or Ethernet. NIXL, in conjunction with the NVIDIA Dynamo policy engine, automatically chooses the best backend connection and abstracts away the differences between multiple types of memory and storage. This is accomplished through generalized “memory sections” which can be HBM, DRAM, local SSD, or networked storage (Block, Object, or File). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Structure of the Course\n",
    "\n",
    "The objective of this section is to give you a high-level understanding of the course structure. This course is self-contained and provides hands-on experience with NVIDIA Dynamo deployed on Azure Kubernetes Service (AKS). \n",
    "\n",
    "### Lab Environment\n",
    "This course uses a pre-configured **Azure Kubernetes Service (AKS)** cluster with:\n",
    "- **CPU Node Pool**: Standard_D16ds_v4 (16 vCPUs, 64 GB RAM) for control plane operations\n",
    "- **GPU Node Pool**: Standard_NC40ads_H100_v5 with 2x H100 GPUs per node for inference workloads\n",
    "- **NVIDIA GPU Operator** pre-installed for GPU resource management\n",
    "- **Nginx Ingress Controller** for external access\n",
    "- **Dynamic scaling** - GPU nodes scale automatically based on workload demand\n",
    "\n",
    "### Prerequisites\n",
    "- Basic understanding of Kubernetes concepts (pods, services, deployments)\n",
    "- Familiarity with container technologies\n",
    "- Basic knowledge of machine learning inference concepts\n",
    "- **Hugging Face token** (for accessing models)\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "0. [**Course Overview**](Dynamo_00_Course_Overview.ipynb) (*this* notebook)<br>\n",
    "    - Introduction to NVIDIA Dynamo\n",
    "    - Course structure and objectives\n",
    "    - Lab environment setup\n",
    "    - Prerequisites and requirements\n",
    "<br><br>\n",
    "\n",
    "1. [**Architecture & Lab Overview**](Dynamo_01_Architecture_and_Lab_Overview.ipynb)<br>\n",
    "    - **Dynamo Architecture**\n",
    "      - Core components and design principles\n",
    "      - Disaggregated serving concepts\n",
    "      - KV Cache Management (KVBM)\n",
    "      - Request routing and scheduling\n",
    "    - **Lab Infrastructure**\n",
    "      - AKS cluster configuration\n",
    "      - GPU resource management\n",
    "      - Monitoring setup\n",
    "<br><br>\n",
    "\n",
    "2. [**vLLM Aggregated Deployment**](Dynamo_02_vLLM_Agg_Deployment.ipynb)<br>\n",
    "    - **Introduction & Setup**\n",
    "      - Environment configuration\n",
    "      - Deployment prerequisites\n",
    "    - **Standard Deployment**\n",
    "      - Basic configuration\n",
    "      - Service deployment\n",
    "      - Testing and validation\n",
    "    - **Router Deployment**\n",
    "      - Cache-aware routing\n",
    "      - Performance optimization\n",
    "    - **Performance Analysis**\n",
    "      - Benchmarking setup\n",
    "      - Results visualization\n",
    "<br><br>\n",
    "\n",
    "3. [**vLLM Disaggregated Deployment**](Dynamo_03_vLLM_disAgg_Deployment.ipynb)<br>\n",
    "    - **Introduction & Setup**\n",
    "      - Disaggregated architecture\n",
    "      - Environment configuration\n",
    "    - **Standard Deployment**\n",
    "      - Deployment configuration\n",
    "      - Service deployment\n",
    "      - Performance monitoring\n",
    "    - **Router Deployment**\n",
    "      - Router architecture\n",
    "      - Configuration & testing\n",
    "    - **Performance Analysis**\n",
    "      - Benchmark execution\n",
    "      - Results comparison\n",
    "<br><br>\n",
    "\n",
    "### Expected Learning Outcomes\n",
    "\n",
    "By completing this course, you will:\n",
    "- Master NVIDIA Dynamo architecture concepts and implementation patterns\n",
    "- Deploy and manage production-ready AI inference infrastructure on Kubernetes\n",
    "- Optimize LLM serving performance using advanced techniques like disaggregated serving\n",
    "- Implement comprehensive monitoring and observability for GPU workloads\n",
    "- Benchmark and analyze different deployment strategies for your specific use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"./images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
