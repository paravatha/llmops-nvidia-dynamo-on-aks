{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a> \n",
    " \n",
    "# 3.0 vLLM Disaggregated Deployment with Dynamo\n",
    "\n",
    "In this notebook, you'll learn how to deploy Large Language Models (LLMs) using NVIDIA Dynamo with vLLM disaggregated backend. We'll explore disaggregated deployment patterns where prefill and decode operations are separated into specialized workers for optimal performance.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Deploy vLLM models using Dynamo's disaggregated configuration\n",
    "- Understand prefill/decode worker separation for improved performance\n",
    "- Implement disaggregated routing with specialized workers\n",
    "- Monitor deployment status and troubleshoot issues\n",
    "- Conduct performance benchmarking and analysis\n",
    "- Compare disaggregated vs aggregated deployment strategies\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "**[3.1 Introduction & Setup](#31-introduction--setup)**\n",
    "- [3.1.1 Dynamo Inference Graphs & Architecture](#311-dynamo-inference-graphs--architecture)\n",
    "- [3.1.2 Environment Configuration](#312-environment-configuration)\n",
    "\n",
    "**[3.2 Standard Disaggregated Deployment](#32-standard-disaggregated-deployment)**\n",
    "- [3.2.1 Deployment Configuration](#321-deployment-configuration)\n",
    "- [3.2.2 Service Deployment & Testing](#322-service-deployment--testing)\n",
    "- [3.2.3 Performance Monitoring](#323-performance-monitoring)\n",
    "\n",
    "**[3.3 Disaggregated Router Deployment](#33-disaggregated-router-deployment)**\n",
    "- [3.3.1 Router Architecture & Benefits](#331-router-architecture--benefits)\n",
    "- [3.3.2 Router Configuration & Deployment](#332-router-configuration--deployment)\n",
    "- [3.3.3 Router Testing & Validation](#333-router-testing--validation)\n",
    "\n",
    "**[3.4 Performance Analysis](#34-performance-analysis)**\n",
    "- [3.4.1 Dataset Generation](#341-dataset-generation)\n",
    "- [3.4.2 Benchmark Execution](#342-benchmark-execution)\n",
    "- [3.4.3 Results & Visualization](#343-results--visualization)\n",
    "\n",
    "**[3.5 Summary & Next Steps](#35-summary--next-steps)** \n",
    " \n",
    "--- \n",
    " \n",
    "## 3.1 Introduction & Setup\n",
    " \n",
    "### 3.1.1 Deployment Configurations \n",
    " \n",
    "In this notebook, we'll implement and compare two deployment strategies: \n",
    " \n",
    "#### 3.1.1.1 **Standard Aggregated Deployment** (`vllm-disagg`) \n",
    "- **Use Case**: Development and moderate-scale inference \n",
    "- **Architecture**: Single-node serving with unified frontend \n",
    "- **Benefits**: Simplified deployment and management \n",
    " \n",
    "#### 3.1.1.2 **KV Cache-Aware Router Deployment** (`vllm-disagg-router`) \n",
    "- **Use Case**: Production environments requiring intelligent routing \n",
    "- **Architecture**: Multiple workers with cache-aware load balancing \n",
    "- **Benefits**: Improved cache efficiency and reduced latency \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[Open Prometheus!](/prom/graph) \n",
    " \n",
    "[Open Grafana!](/grafana/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    " \n",
    "## 3.2 Environment Setup \n",
    " \n",
    "### 3.2.1 Prerequisites Check \n",
    " \n",
    "Before we begin, let's ensure our environment is properly configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "from huggingface_hub import HfApi, login\n",
    "from IPython.display import Markdown, display\n",
    "from utils.aiperf import create_benchmark_comparison_visualization\n",
    "from utils.hf_api import check_model_access, check_token_validity, get_hf_token\n",
    "from utils.k8s_helpers import wait_for_pods_ready\n",
    "from utils.paths import CONFIGS_DIR, DEPLOYMENT_DIR, ensure_dirs_exist\n",
    "from utils.util import image_model_replacement\n",
    "\n",
    "# Configure deployment parameters\n",
    "NAMESPACE = \"dynamo-cloud\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
    "VLLM_IMAGE = \"nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.5.0\"\n",
    "\n",
    "# Ensure required directories exist\n",
    "ensure_dirs_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 3.2.2 HF Token Check \n",
    "We are using Hugging Face token to download the Tokenizer and the Model.\n",
    "\n",
    "Read here on creating token: https://huggingface.co/docs/hub/en/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "token = get_hf_token()\n",
    "\n",
    "# Step 1: Validate token\n",
    "if not check_token_validity(api, token):\n",
    "    token = input(\"Please re-enter a valid Hugging Face token: \").strip()\n",
    "    if not check_token_validity(api, token):\n",
    "        print(\"Exiting: Invalid token provided twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check model access\n",
    "check_model_access(api, token, MODEL_NAME)\n",
    "\n",
    "# Step 3: Hugging Face login\n",
    "login(token=token)\n",
    "print(\"Logged in to Hugging Face Hub via `login()`.\")\n",
    "\n",
    "# Step 4: Create K8s secret\n",
    "!kubectl create secret generic hf-token-secret --from-literal=HF_TOKEN=$token -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 3.2.3 Network Configuration \n",
    " \n",
    "Let's get the load balancer IP address that we'll use to access our deployed services. This IP will be used for external access to our vLLM deployments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the load balancer IP\n",
    "load_balancer_ip_result = !kubectl get svc ingress-nginx-controller -n nginx-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n",
    "load_balancer_ip = load_balancer_ip_result[0]\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   Namespace: {NAMESPACE}\")\n",
    "print(f\"   Load Balancer IP: {load_balancer_ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 3.2.4 Delete exisiting Deployments\n",
    " \n",
    "Before we proceed with the deployments, let's clean up any existing aggregated deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f {CONFIGS_DIR}/vllm-agg.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f {CONFIGS_DIR}/vllm-agg-router.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 Standard Disaggregated Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### 3.3.1 Deploy Persistent Volume Claim \n",
    " \n",
    "Let's create the PVC that will store our model files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PVC_DIR}/vllm-disagg.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the PVC configuration\n",
    "!kubectl apply -f {PVC_DIR}/vllm-disagg.yaml -n $NAMESPACE\n",
    "!kubectl get pvc vllm-disagg-pvc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 3.3.2 Deploying vLLM disaggregated\n",
    "\n",
    "Now let's deploy our first configuration - the standard vLLM disaggregated deployment. This setup provides a simple, unified serving architecture ideal for development and moderate-scale inference workloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "In disaggregated deployment, we separate the inference pipeline into specialized workers: \n",
    " \n",
    "#### **VllmPrefillWorker** (Input Processing) \n",
    "- **Purpose**: Handles input token processing and attention computation \n",
    "- **Workload**: Compute-intensive operations for prompt encoding \n",
    "- **Resource Usage**: High GPU compute, moderate memory \n",
    "- **Command**: `--is-prefill-worker` flag enables prefill specialization \n",
    " \n",
    "#### **VllmDecodeWorker** (Token Generation) \n",
    "- **Purpose**: Handles output token generation and autoregressive decoding \n",
    "- **Workload**: Memory-intensive operations with KV cache management \n",
    "- **Resource Usage**: Moderate GPU compute, high memory for cache \n",
    "- **Command**: Standard worker mode optimized for decode operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    " \n",
    "#### High-level flow: \n",
    "<center><img src=\"images/dynamo/flows/vllm-disagg.png\" width=\"500px\"></center> \n",
    " \n",
    " \n",
    "#### Deployment Architecture: \n",
    "<center><img src=\"images/dynamo/flows/vllm-disagg-aks.png\" width=\"1000px\"></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configured YAML for vLLM deployment\n",
    "\n",
    "# Get source and destination paths\n",
    "image_model_replacement(\n",
    "    source_yaml_path=f\"{DEPLOYMENT_DIR}/vllm-disagg.yaml\",\n",
    "    destination_directory=CONFIGS_DIR,\n",
    "    vllm_image=VLLM_IMAGE,\n",
    "    model_name=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated configuration\n",
    "print(\"\\nüìÑ Generated Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "!cat {CONFIGS_DIR}/'vllm-disagg.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the vLLM configuration\n",
    "!kubectl apply -f {CONFIGS_DIR}/vllm-disagg.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show deployment status\n",
    "!kubectl get dynamographdeployment vllm-disagg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Sample output for the pods: \n",
    "```\n",
    "NAME                                             READY   STATUS    RESTARTS   AGE\n",
    "vllm-disagg-frontend-58d854b979-4hdvk            1/1     Running   0          2m9s\n",
    "vllm-disagg-vllmdecodeworker-7bdb9bbb59-w7dq8    1/1     Running   0          2m8s\n",
    "vllm-disagg-vllmprefillworker-59589c4c59-8pmzr   1/1     Running   0          2m8s\n",
    "vllm-disagg-vllmprefillworker-59589c4c59-bg5c9   1/1     Running   0          2m8s\n",
    "vllm-disagg-vllmprefillworker-59589c4c59-qgmc4   1/1     Running   0          2m8s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-disagg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_disagg_vllmprefillworker_name = !kubectl get pods -n $NAMESPACE | grep vllm-disagg-vllmprefillworker | cut -d' ' -f1\n",
    "\n",
    "!kubectl describe pods {vllm_disagg_vllmprefillworker_name[0]}  -n dynamo-cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for vLLM pods to be ready\n",
    "wait_for_pods_ready(\"vllm-disagg\", NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-disagg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment vllm-disagg-frontend --type=ClusterIP --port=8000 --target-port=8000 -n $NAMESPACE\n",
    "!kubectl get svc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 3.3.3 Setting up External Access \n",
    " \n",
    "Now that our vLLM service is running, we need to configure ingress to allow external access. The ingress controller will route HTTP requests to our vLLM frontend service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {INGRESS_DIR}/vllm-disagg-ingress.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deploying Ingress for External Access...\")\n",
    "\n",
    "# Deploy the ingress configuration\n",
    "!kubectl apply -f {INGRESS_DIR}/vllm-disagg-ingress.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 3.3.4 Testing the Standard vLLM Deployment \n",
    " \n",
    "Now that our vLLM service is deployed and accessible, let's test it to ensure everything is working correctly. We'll start with basic connectivity tests and then move on to inference testing. \n",
    " \n",
    "#### 3.3.4.1 Basic Connectivity Tests \n",
    " \n",
    "First, let's verify that our service is accessible and responding to requests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-disagg/v1/models\" | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### 3.3.4.2 Chat Completion Test \n",
    " \n",
    "Now let's test the actual inference capability by sending a chat completion request to our vLLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Configure the chat completion request\n",
    "endpoint = f\"http://{load_balancer_ip}/vllm-disagg/v1/chat/completions\"\n",
    "headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a polite and respectful chatbot helping people plan a vacation.\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": \"What is the capital of Germany?\", \"role\": \"user\"},\n",
    "    ],\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 150,\n",
    "    \"top_p\": 1,\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "llm_response = requests.post(endpoint, headers=headers, json=data, timeout=60)\n",
    "\n",
    "if llm_response.status_code == 200:\n",
    "    response_data = llm_response.json()\n",
    "    print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### 3.3.4.3 Metrics Monitoring \n",
    " \n",
    "Let's check the metrics endpoint to see performance statistics for our vLLM service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-disagg/metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.4 Performance Analysis\n",
    " \n",
    "### 3.4.1 Generating Test Dataset \n",
    " \n",
    "Let's create a comprehensive test dataset that simulates realistic LLM inference patterns. \n",
    "\n",
    "```\n",
    "we will not be generating dataset as we will be using the dataset created in Dynamo_02_vLLM_Agg_Deployment.ipynb. If you have not created there, you can uncomment below. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate test dataset for benchmarking\n",
    "# success = generate_mooncake_dataset(\n",
    "#     time_duration=60,\n",
    "#     request_rate_min=3,\n",
    "#     request_rate_max=5,\n",
    "#     request_rate_period=30,\n",
    "#     isl1=250,\n",
    "#     osl1=50,\n",
    "#     isl2=500,\n",
    "#     osl2=100,\n",
    "#     output_file=\"results/dataset.jsonl\"\n",
    "# )\n",
    "\n",
    "# if not success:\n",
    "#     print(\"Failed to generate test dataset\")\n",
    "#     raise Exception(\"Dataset generation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 3.4.2 Benchmarking Standard vLLM Deployment \n",
    " \n",
    "Let's start with benchmarking our standard deployment to establish baseline performance. \n",
    "\n",
    "<p style=\"color:red\">Copy the below printed command and run it in the terminal:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f'aiperf profile -m \"{MODEL_NAME}\" --endpoint-type \"chat\" --url \"http://{load_balancer_ip}/vllm-disagg\" --input-file \"/dli/task/results/dataset.jsonl\" --custom-dataset-type \"mooncake_trace\" --fixed-schedule --artifact-dir \"/dli/task/results/standard-benchmark-disagg\" --streaming'\n",
    "\n",
    "\n",
    "# Display the command for reference\n",
    "print(\n",
    "    \"Please make sure you have run the following command in terminal before proceeding:\\n\"\n",
    ")\n",
    "# Display as shell block\n",
    "display(Markdown(f\"```bash\\n{command}\\n```\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ask for confirmation\n",
    "response = input(\"Have you run this command? (yes/no): \").strip().lower()\n",
    "\n",
    "# Conditional execution\n",
    "if response == \"yes\":\n",
    "    print(\"Great! You can proceed.\")\n",
    "else:\n",
    "    print(\"Please run the command in terminal first before proceeding.\")\n",
    "    # Optionally, stop execution\n",
    "    import sys\n",
    "\n",
    "    sys.exit(\"Execution stopped. Run the command first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Check the Grafana Dashboard\n",
    "[Open Grafana!](/grafana/) -> Dynamo Dashboard\n",
    "\n",
    "<center><img src=\"images/dynamo-grafana-dashboard.png\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 3.4.3 Cleanup disaggregated deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f dynamo-deployments/vllm-disagg.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.5 Disaggregated Router Deployment\n",
    "\n",
    "### 3.5.1 Router Architecture & Benefits\n",
    "\n",
    "Now let's deploy our second configuration - the disaggregated router deployment. This advanced setup provides intelligent routing with multiple specialized workers for enhanced performance and scalability.\n",
    "\n",
    "Disaggregated routing is an optimization technique that:\n",
    "- **Separates prefill and decode operations** across specialized worker instances\n",
    "- **Routes requests intelligently** to appropriate worker types based on operation\n",
    "- **Optimizes resource allocation** by matching workload characteristics to worker capabilities\n",
    "- **Improves throughput** through parallel processing of different operation types "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### 3.5.2 Router Storage Configuration \n",
    " \n",
    "The router deployment requires its own storage volume for model files. Let's create a dedicated PVC for the router configuration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PVC_DIR}/vllm-disagg-router.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the router PVC configuration\n",
    "!kubectl apply -f {PVC_DIR}/vllm-disagg-router.yaml -n $NAMESPACE\n",
    "!kubectl get pvc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 3.5.3 Deploying the Router Service \n",
    " \n",
    "Let's visualize how KV cache-aware routing works compared to standard routing: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    " \n",
    "#### High-level flow: \n",
    "<center><img src=\"images/dynamo/flows/vllm-disagg-router.png\" width=\"500px\"></center> \n",
    " \n",
    " \n",
    "#### Deployment Architecture: \n",
    "<center><img src=\"images/dynamo/flows/vllm-disagg-router-aks.png\" width=\"1000px\"></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    " \n",
    "Now let's deploy the disaggregated router configuration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configured YAML for router deployment\n",
    "success = image_model_replacement(\n",
    "    source_yaml_path=f\"{DEPLOYMENT_DIR}/vllm-disagg-router.yaml\",\n",
    "    destination_directory=CONFIGS_DIR,\n",
    "    vllm_image=VLLM_IMAGE,\n",
    "    model_name=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {CONFIGS_DIR}/vllm-disagg-router.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the router configuration\n",
    "!kubectl apply -f {CONFIGS_DIR}/vllm-disagg-router.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show deployment status\n",
    "print(\"\\nDeployment Status:\")\n",
    "!kubectl get dynamographdeployment -n $NAMESPACE\n",
    "\n",
    "print(\"\\nRouter Pod Status:\")\n",
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-disagg-router -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for router pods to be ready\n",
    "wait_for_pods_ready(\"vllm-disagg-router\", NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-disagg-router -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment vllm-disagg-router-frontend --type=ClusterIP --port=8000 --target-port=8000 -n $NAMESPACE\n",
    "!kubectl get svc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### 3.5.4 Router External Access Configuration \n",
    " \n",
    "Now let's configure ingress for the router deployment to enable external access with the `/vllm-disagg-router` path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê Deploying Router Ingress...\")\n",
    "\n",
    "# Deploy the router ingress configuration\n",
    "!kubectl apply -f {INGRESS_DIR}/vllm-disagg-router-ingress.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### 3.5.5 Router Testing & Validation\n",
    "\n",
    "Let's test our disaggregated router deployment to ensure it's working correctly: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### 3.5.5.1 Basic Connectivity Tests \n",
    " \n",
    "First, let's verify that our service is accessible and responding to requests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-disagg-router/v1/models\" | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### 3.5.5.2 Router Chat Completion Test \n",
    " \n",
    "Let's test the inference capability of our router deployment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"http://{load_balancer_ip}/vllm-disagg-router/v1/chat/completions\"\n",
    "router_response = requests.post(endpoint, headers=headers, json=data, timeout=60)\n",
    "response_data = router_response.json()\n",
    "print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## 3.6 Benchmark Execution\n",
    "\n",
    "### 3.6.1 Benchmark vLLM Disaggregated Router\n",
    "\n",
    "Now let's benchmark our deployments to compare their performance. \n",
    "\n",
    "<p style=\"color:red\">Copy the below printed command and run it in the terminal:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "command = f'aiperf profile -m \"{MODEL_NAME}\" --endpoint-type \"chat\" --url \"http://{load_balancer_ip}/vllm-disagg-router\" --input-file \"/dli/task/results/dataset.jsonl\" --custom-dataset-type \"mooncake_trace\" --fixed-schedule --artifact-dir \"/dli/task/results/router-benchmark-disagg\" --streaming'\n",
    "\n",
    "\n",
    "# Display the command for reference\n",
    "print(\n",
    "    \"Please make sure you have run the following command in terminal before proceeding:\\n\"\n",
    ")\n",
    "# Display as shell block\n",
    "display(Markdown(f\"```bash\\n{command}\\n```\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ask for confirmation\n",
    "response = input(\"Have you run this command? (yes/no): \").strip().lower()\n",
    "\n",
    "# Conditional execution\n",
    "if response == \"yes\":\n",
    "    print(\"Great! You can proceed.\")\n",
    "else:\n",
    "    print(\"Please run the command in terminal first before proceeding.\")\n",
    "    # Optionally, stop execution\n",
    "    import sys\n",
    "\n",
    "    sys.exit(\"Execution stopped. Run the command first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "#### Check the Grafana Dashboard\n",
    "[Open Grafana!](/grafana/) -> Dynamo Dashboard\n",
    "\n",
    "<center><img src=\"images/dynamo-grafana-dashboard.png\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "\n",
    "### 3.6.2 Results & Visualization\n",
    "\n",
    "Now let's analyze the benchmark results from both deployments and create comprehensive visualizations comparing their performance characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for disaggregated deployments\n",
    "comparison_fig = create_benchmark_comparison_visualization(\n",
    "    standard_dir=\"results/standard-benchmark-disagg\",\n",
    "    router_dir=\"results/router-benchmark-disagg\",\n",
    "    deployment_labels=(\"Standard Disagg\", \"Router Disagg\"),\n",
    "    output_filename=\"disagg_benchmark_comparison.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "## Key Takeaways: Disaggregated Deployment\n",
    "\n",
    "**What is Disaggregated Deployment?**\n",
    "- Separates the **prefill** (prompt processing) and **decode** (token generation) phases into independent components\n",
    "- Uses a **KV Cache-aware router** to intelligently direct requests to backends with warm caches\n",
    "- Enables **independent scaling** of each component based on workload characteristics\n",
    "\n",
    "**Performance Benefits:**\n",
    "- **Reduced Latency**: Router directs requests to backends with relevant cached data, minimizing redundant computations\n",
    "- **Higher Throughput**: Optimized resource allocation for prefill vs. decode operations\n",
    "- **Better Resource Utilization**: Scale prefill and decode independently based on actual demand\n",
    "\n",
    "**When to Use:**\n",
    "- Production environments requiring consistent low latency\n",
    "- Applications with repeated or similar prompts (e.g., chatbots, RAG systems)\n",
    "- High-traffic scenarios where intelligent routing provides measurable gains\n",
    "\n",
    "---\n",
    "\n",
    "### Feedback and Questions\n",
    "\n",
    "We hope this lab provided valuable insights into deploying high-performance LLM inference systems. For questions, issues, or suggestions, please reach out to your instructor or consult the course materials.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
