{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a> \n",
    " \n",
    "# 2.0 vLLM Aggregated Deployment with Dynamo \n",
    " \n",
    "In this notebook, you'll learn how to deploy Large Language Models (LLMs) using NVIDIA Dynamo with vLLM backend. We'll explore two different deployment patterns, perform performance testing, and compare results. \n",
    " \n",
    "## Learning Objectives \n",
    " \n",
    "By the end of this notebook, you will be able to: \n",
    "- Deploy vLLM models using Dynamo's aggregated configuration \n",
    "- Implement KV Cache-aware routing for improved performance \n",
    "- Monitor deployment status and troubleshoot issues \n",
    "- Conduct performance benchmarking and analysis \n",
    "- Compare different deployment strategies \n",
    " \n",
    "--- \n",
    " \n",
    "## Table of Contents \n",
    " \n",
    "**[2.1 Introduction & Setup](#21-introduction--setup)** \n",
    "- [2.1.1 Dynamo Inference Graphs & Configurations](#211-dynamo-inference-graphs--configurations) \n",
    "- [2.1.2 Environment Setup & Prerequisites](#212-environment-setup--prerequisites)\n",
    "\n",
    "**[2.2 Aggregated vLLM Deployment](#22-standard-vllm-deployment)** \n",
    "- [2.2.1 Deployment Configuration](#221-deployment-configuration) \n",
    "- [2.2.2 Service Deployment & Testing](#222-service-deployment--testing)\n",
    "\n",
    "**[2.3 KV Cache-Aware Router Deployment](#23-kv-cache-aware-router-deployment)** \n",
    "- [2.3.1 Router Architecture & Benefits](#231-router-architecture--benefits) \n",
    "- [2.3.2 Router Deployment & Testing](#232-router-deployment--testing)\n",
    " \n",
    "**[2.4 Performance Analysis](#24-performance-analysis)** \n",
    "- [2.4.1 Benchmark Setup & Execution](#241-benchmark-setup--execution) \n",
    "- [2.4.2 Results & Visualization](#242-results--visualization)\n",
    " \n",
    "**[2.5 Summary & Next Steps](#25-summary--next-steps)** \n",
    " \n",
    "--- \n",
    " \n",
    "## 2.1 Introduction \n",
    " \n",
    "### 2.1.1 What are Dynamo Inference Graphs? \n",
    " \n",
    "Dynamo inference graphs represent complete LLM serving pipelines with interconnected components: \n",
    " \n",
    "| Component | Description | Role | \n",
    "|-----------|-------------|------| \n",
    "| **Frontend** | HTTP request handler | Routes requests and manages responses | \n",
    "| **Prefill Workers** | Input token processors | Handle compute-intensive prefill operations | \n",
    "| **Decode Workers** | Output token generators | Manage memory-intensive decode operations | \n",
    "| **Load Balancers** | Request distributors | Optimize workload distribution | \n",
    " \n",
    "### 2.1.2 Deployment Configurations \n",
    " \n",
    "In this notebook, we'll implement and compare two deployment strategies: \n",
    " \n",
    "#### 2.1.2.1 **Standard Aggregated Deployment** (`vllm-agg`) \n",
    "- **Use Case**: Development and moderate-scale inference \n",
    "- **Architecture**: Single-node serving with unified frontend \n",
    "- **Benefits**: Simplified deployment and management \n",
    "#### 2.1.2.2 **KV Cache-Aware Router Deployment** (`vllm-agg-router`) \n",
    "- **Use Case**: Production environments requiring intelligent routing \n",
    "- **Architecture**: Multiple workers with cache-aware load balancing \n",
    "- **Benefits**: Improved cache efficiency and reduced latency \n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[Open Prometheus!](/prom/graph) \n",
    " \n",
    "[Open Grafana!](/grafana/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    " \n",
    "## 2.2 Environment Setup \n",
    " \n",
    "### 2.2.1 Prerequisites Check \n",
    " \n",
    "Before we begin, let's ensure our environment is properly configured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "from huggingface_hub import HfApi, login\n",
    "from IPython.display import Markdown, display\n",
    "from utils.aiperf import create_benchmark_comparison_visualization\n",
    "from utils.data_generation import generate_mooncake_dataset\n",
    "from utils.hf_api import check_model_access, check_token_validity, get_hf_token\n",
    "from utils.k8s_helpers import wait_for_pods_ready\n",
    "from utils.paths import CONFIGS_DIR, DEPLOYMENT_DIR, ensure_dirs_exist\n",
    "from utils.util import image_model_replacement\n",
    "\n",
    "# Configure deployment parameters\n",
    "NAMESPACE = \"dynamo-cloud\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\"\n",
    "VLLM_IMAGE = \"nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.5.0\"\n",
    "\n",
    "# Ensure required directories exist\n",
    "ensure_dirs_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2.2.2 HF Token Check \n",
    "We are using Hugging Face token to download the Tokenizer and the Model.\n",
    "\n",
    "Read here on creating token: https://huggingface.co/docs/hub/en/security-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "token = get_hf_token()\n",
    "\n",
    "# Step 1: Validate token\n",
    "if not check_token_validity(api, token):\n",
    "    token = input(\"Please re-enter a valid Hugging Face token: \").strip()\n",
    "    if not check_token_validity(api, token):\n",
    "        print(\"Exiting: Invalid token provided twice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Check model access\n",
    "check_model_access(api, token, MODEL_NAME)\n",
    "\n",
    "# Step 3: Hugging Face login\n",
    "login(token=token)\n",
    "print(\"Logged in to Hugging Face Hub via `login()`.\")\n",
    "\n",
    "# Step 4: Create K8s secret\n",
    "!kubectl create secret generic hf-token-secret --from-literal=HF_TOKEN=$token -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 2.2.3 Network Configuration \n",
    " \n",
    "Let's get the load balancer IP address that we'll use to access our deployed services. This IP will be used for external access to our vLLM deployments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the load balancer IP\n",
    "load_balancer_ip_result = !kubectl get svc ingress-nginx-controller -n nginx-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n",
    "load_balancer_ip = load_balancer_ip_result[0]\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "print(f\"   Namespace: {NAMESPACE}\")\n",
    "print(f\"   Load Balancer IP: {load_balancer_ip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    "## 2.3 vLLM Aggregated Deployment \n",
    " \n",
    "Now let's deploy our first configuration - the standard vLLM aggregated deployment. This setup provides a simple, unified serving architecture ideal for development and moderate-scale inference workloads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.3.1 Persistent Volume Configuration \n",
    " \n",
    "Before deploying our vLLM service, we need to create persistent storage for model files. This PersistentVolumeClaim (PVC) will store the downloaded model weights and be shared across our deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PVC_DIR}/vllm-agg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    " Let's create the PVC that will store our model files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the PVC configuration\n",
    "!kubectl apply -f {PVC_DIR}/vllm-agg.yaml -n $NAMESPACE\n",
    "!kubectl get pvc vllm-agg-pvc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 2.3.2 Deploying the vLLM Service "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    " \n",
    "#### High-level flow: \n",
    "<center><img src=\"images/dynamo/flows/vllm-agg.png\" width=\"350px\"></center> \n",
    " \n",
    " \n",
    "#### Deployment Architecture: \n",
    "<center><img src=\"images/dynamo/flows/vllm-agg-aks.png\" width=\"1000px\"></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The following configuration creates a simple aggregated deployment with: \n",
    "- **Frontend**: Handles HTTP requests and routes them to workers \n",
    "- **VllmDecodeWorker**: Processes both prefill and decode operations \n",
    "- **Model**: Uses Qwen2.5-32B-Instruct-GPTQ-Int4 as our example LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configured YAML for vLLM deployment\n",
    "\n",
    "# Get source and destination paths\n",
    "image_model_replacement(\n",
    "    source_yaml_path=f\"{DEPLOYMENT_DIR}/vllm-agg.yaml\",\n",
    "    destination_directory=CONFIGS_DIR,\n",
    "    vllm_image=VLLM_IMAGE,\n",
    "    model_name=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the generated configuration\n",
    "print(\"\\nüìÑ Generated Configuration:\")\n",
    "print(\"=\" * 60)\n",
    "!cat {CONFIGS_DIR}/'vllm-agg.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the vLLM configuration\n",
    "!kubectl apply -f {CONFIGS_DIR}/vllm-agg.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show deployment status\n",
    "!kubectl get dynamographdeployment vllm-agg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Sample output:\n",
    "``` \n",
    "NAME                                         READY   STATUS    RESTARTS   AGE\n",
    "vllm-agg-frontend-5dbbfb4c85-r8m85           0/1     Pending   0          7s\n",
    "vllm-agg-vllmdecodeworker-85fc89dcf5-8kw59   0/1     Pending   0          7s\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-agg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The pods must be pending as the node is scaling up. The following warning is ok. \n",
    "``` \n",
    " Warning FailedScheduling 3m24s default-scheduler 0/1 nodes are available: 1 node(s) didn't match Pod's node affinity/selector. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling. \n",
    " Normal TriggeredScaleUp 3m14s cluster-autoscaler pod triggered scale-up: [{aks-h100-22443994-vmss 0->1 (max: 2)}] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_agg_vllmdecodeworker_name = !kubectl get pods -n $NAMESPACE | grep vllm-agg-vllmdecodeworker | cut -d' ' -f1\n",
    "\n",
    "!kubectl describe pods {vllm_agg_vllmdecodeworker_name[0]} -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for vLLM pods to be ready\n",
    "wait_for_pods_ready(\"vllm-agg\", NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-agg -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment vllm-agg-frontend --type=ClusterIP --port=8000 --target-port=8000 -n $NAMESPACE\n",
    "!kubectl get svc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 2.3.3 Setting up External Access \n",
    " \n",
    "Now that our vLLM service is running, we need to configure ingress to allow external access. The ingress controller will route HTTP requests to our vLLM frontend service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {INGRESS_DIR}/vllm-agg-ingress.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the ingress configuration\n",
    "!kubectl apply -f {INGRESS_DIR}/vllm-agg-ingress.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Wait a moment for ingress to be processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 2.3.4 Testing the Standard vLLM Deployment \n",
    " \n",
    "Now that our vLLM service is deployed and accessible, let's test it to ensure everything is working correctly. We'll start with basic connectivity tests and then move on to inference testing. \n",
    " \n",
    "#### 2.3.4.1 Basic Connectivity Tests \n",
    " \n",
    "First, let's verify that our service is accessible and responding to requests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-agg/v1/models\" | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### 2.3.4.2 Chat Completion Test \n",
    " \n",
    "Now let's test the actual inference capability by sending a chat completion request to our vLLM service: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Configure the chat completion request\n",
    "endpoint = f\"http://{load_balancer_ip}/vllm-agg/v1/chat/completions\"\n",
    "headers = {\"accept\": \"application/json\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a polite and respectful chatbot helping people plan a vacation.\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": \"What is the capital of Germany?\", \"role\": \"user\"},\n",
    "    ],\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 150,\n",
    "    \"top_p\": 1,\n",
    "    \"stream\": False,\n",
    "}\n",
    "\n",
    "llm_response = requests.post(endpoint, headers=headers, json=data, timeout=60)\n",
    "\n",
    "if llm_response.status_code == 200:\n",
    "    response_data = llm_response.json()\n",
    "    print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### 2.3.4.3 Metrics Monitoring \n",
    " \n",
    "Let's check the metrics endpoint to see performance statistics for our vLLM service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-agg/metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "--- \n",
    " \n",
    "## 2.4 Part 2: KV Cache-Aware Router Deployment \n",
    " \n",
    "Now let's deploy our second configuration - the KV Cache-aware router deployment. This advanced setup provides intelligent routing that considers Key-Value cache states to minimize cache misses and improve overall performance. \n",
    " \n",
    "### 2.4.1 What is KV Cache-Aware Routing? \n",
    " \n",
    "KV Cache-aware routing is an optimization technique that: \n",
    "- **Tracks cache states** across multiple worker instances \n",
    "- **Routes requests intelligently** to workers with relevant cached data \n",
    "- **Reduces cache misses** by leveraging previously computed key-value pairs \n",
    "- **Improves latency** for similar or continuing conversations \n",
    " \n",
    "#### Key Benefits: \n",
    "- **Better Cache Utilization**: Maximizes reuse of cached computations \n",
    "- **Reduced Latency**: Faster response times for cache hits \n",
    "- **Cost Efficiency**: Lower compute requirements through cache reuse \n",
    "- **Scalability**: Better performance under high load \n",
    " \n",
    "### 2.4.2 Configuration Comparison \n",
    "\n",
    "Enables KV cache-aware routing logic:\n",
    "```\n",
    " \"--router-mode kv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### 2.4.3 Router Storage Configuration \n",
    " \n",
    "The router deployment requires its own storage volume for model files. Let's create a dedicated PVC for the router configuration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {PVC_DIR}/vllm-agg-router.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the router PVC configuration\n",
    "!kubectl apply -f {PVC_DIR}/vllm-agg-router.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pvc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 2.4.4 KV Cache-Aware Routing Architecture \n",
    " \n",
    "Let's visualize how KV cache-aware routing works compared to standard routing: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    " \n",
    "#### High-level flow: \n",
    "<center><img src=\"images/dynamo/flows/vllm-agg-router.png\" width=\"500px\"></center> \n",
    " \n",
    " \n",
    "#### Deployment Architecture: \n",
    "<center><img src=\"images/dynamo/flows/vllm-agg-router-aks.png\" width=\"1000px\"></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 2.4.5 Deploying the Router Service \n",
    " \n",
    "Now let's deploy the KV Cache-aware router configuration: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configured YAML for router deployment\n",
    "success = image_model_replacement(\n",
    "    source_yaml_path=f\"{DEPLOYMENT_DIR}/vllm-agg-router.yaml\",\n",
    "    destination_directory=CONFIGS_DIR,\n",
    "    vllm_image=VLLM_IMAGE,\n",
    "    model_name=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {CONFIGS_DIR}/vllm-agg-router.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the router configuration\n",
    "!kubectl apply -f {CONFIGS_DIR}/vllm-agg-router.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show deployment status\n",
    "!kubectl get dynamographdeployment -n $NAMESPACE\n",
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-agg-router -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for router pods to be ready\n",
    "wait_for_pods_ready(\"vllm-agg-router\", NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l nvidia.com/dynamo-namespace=vllm-agg-router -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl expose deployment vllm-agg-router-frontend --type=ClusterIP --port=8000 --target-port=8000 -n $NAMESPACE\n",
    "!kubectl get svc -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### 2.4.6 Router External Access Configuration \n",
    " \n",
    "Now let's configure ingress for the router deployment to enable external access with the `/vllm-agg-router` path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê Deploying Router Ingress...\")\n",
    "\n",
    "# Deploy the router ingress configuration\n",
    "!kubectl apply -f {INGRESS_DIR}/vllm-agg-router-ingress.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Wait a moment for ingress to be processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### 2.4.7 Testing the Router Deployment \n",
    " \n",
    "Let's test our KV Cache-aware router deployment to ensure it's working correctly: \n",
    " \n",
    "#### 2.4.7.1 Router Connectivity Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"http://{load_balancer_ip}/vllm-agg-router/v1/models\" | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### 2.4.7.2 Router Chat Completion Test \n",
    " \n",
    "Let's test the inference capability of our router deployment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"http://{load_balancer_ip}/vllm-agg-router/v1/chat/completions\"\n",
    "router_response = requests.post(endpoint, headers=headers, json=data, timeout=60)\n",
    "response_data = router_response.json()\n",
    "print(response_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## 2.5 Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### 2.5.1 Benchmarking Standard vLLM Deployment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test dataset for benchmarking\n",
    "success = generate_mooncake_dataset(\n",
    "    time_duration=60,\n",
    "    request_rate_min=3,\n",
    "    request_rate_max=5,\n",
    "    request_rate_period=30,\n",
    "    isl1=250,\n",
    "    osl1=50,\n",
    "    isl2=500,\n",
    "    osl2=100,\n",
    "    output_file=\"results/dataset.jsonl\",\n",
    ")\n",
    "\n",
    "if not success:\n",
    "    print(\"Failed to generate test dataset\")\n",
    "    raise Exception(\"Dataset generation failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 2.5.2 Benchmarking Standard vLLM Deployment \n",
    " \n",
    "Let's start with benchmarking our standard deployment to establish baseline performance.\n",
    "\n",
    "<p style=\"color:red\">Copy the below printed command and run it in the terminal:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f'aiperf profile -m \"{MODEL_NAME}\" --endpoint-type \"chat\" --url \"http://{load_balancer_ip}/vllm-agg\" --input-file \"/dli/task/results/dataset.jsonl\" --custom-dataset-type \"mooncake_trace\" --fixed-schedule --artifact-dir \"/dli/task/results/standard-benchmark\" --streaming'\n",
    "\n",
    "\n",
    "# Display the command for reference\n",
    "print(\n",
    "    \"Please make sure you have run the following command in terminal before proceeding:\\n\"\n",
    ")\n",
    "# Display as shell block\n",
    "display(Markdown(f\"```bash\\n{command}\\n```\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ask for confirmation\n",
    "response = input(\"Have you run this command? (yes/no): \").strip().lower()\n",
    "\n",
    "# Conditional execution\n",
    "if response == \"yes\":\n",
    "    print(\"Great! You can proceed.\")\n",
    "else:\n",
    "    print(\"Please run the command in terminal first before proceeding.\")\n",
    "    # Optionally, stop execution\n",
    "    import sys\n",
    "\n",
    "    sys.exit(\"Execution stopped. Run the command first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "You will get a screen like below: \n",
    "<center><img src=\"images/ai-perf.png\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### 2.5.3 Benchmarking KV Cache-Aware Router Deployment \n",
    " \n",
    "Now let's benchmark our KV Cache-aware router deployment to compare performance.\n",
    "\n",
    "\n",
    "<p style=\"color:red\">Copy the below printed command and run it in the terminal:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "command = f'aiperf profile -m \"{MODEL_NAME}\" --endpoint-type \"chat\" --url \"http://{load_balancer_ip}/vllm-agg-router\" --input-file \"/dli/task/results/dataset.jsonl\" --custom-dataset-type \"mooncake_trace\" --fixed-schedule --artifact-dir \"/dli/task/results/router-benchmark\" --streaming'\n",
    "\n",
    "\n",
    "# Display the command for reference\n",
    "print(\n",
    "    \"Please make sure you have run the following command in terminal before proceeding:\\n\"\n",
    ")\n",
    "# Display as shell block\n",
    "display(Markdown(f\"```bash\\n{command}\\n```\"))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ask for confirmation\n",
    "response = input(\"Have you run this command? (yes/no): \").strip().lower()\n",
    "\n",
    "# Conditional execution\n",
    "if response == \"yes\":\n",
    "    print(\"Great! You can proceed.\")\n",
    "else:\n",
    "    print(\"Please run the command in terminal first before proceeding.\")\n",
    "    # Optionally, stop execution\n",
    "    import sys\n",
    "\n",
    "    sys.exit(\"Execution stopped. Run the command first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "You will get a screen like below: \n",
    "<center><img src=\"images/ai-perf-vllm-agg-router.png\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### Check the Grafana Dashboard\n",
    "[Open Grafana!](/grafana/) -> Dynamo Dashboard\n",
    "\n",
    "<center><img src=\"images/dynamo-grafana-dashboard.png\" width=\"700px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### 2.5.4 Performance Data Analysis & Visualization \n",
    " \n",
    "Now let's analyze the benchmark results from both deployments and create comprehensive visualizations comparing their performance characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization for aggregated deployments\n",
    "comparison_fig = create_benchmark_comparison_visualization(\n",
    "    standard_dir=\"results/standard-benchmark\",\n",
    "    router_dir=\"results/router-benchmark\",\n",
    "    deployment_labels=(\"Standard Agg\", \"Router Agg\"),\n",
    "    output_filename=\"agg_benchmark_comparison.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## 2.6 Clean Up\n",
    "\n",
    "### 2.6.1 Delete exisiting Deployments\n",
    " \n",
    "Before we proceed with the next notebook, let's clean up any existing aggregated deployments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f {CONFIGS_DIR}/vllm-agg.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f {CONFIGS_DIR}/vllm-agg-router.yaml -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "--- \n",
    "<h2 style=\"color:green;\">Congratulations!</h2> \n",
    " \n",
    "## Key Takeaways: Aggregated Deployment\n",
    "\n",
    "**What is Aggregated Deployment?**\n",
    "- **All-in-one architecture**: Prefill (prompt processing) and decode (token generation) run together in the same pods\n",
    "- **Unified scaling**: All components scale together as a single unit\n",
    "- **Simpler configuration**: Fewer components to manage and monitor\n",
    "\n",
    "**Benefits:**\n",
    "- **Simplicity**: Easier to deploy, configure, and troubleshoot\n",
    "- **Lower Overhead**: No router or coordination between separate components\n",
    "- **Resource Efficiency**: Good for moderate traffic workloads where separation isn't needed\n",
    "- **Faster Time-to-Deploy**: Fewer moving parts means quicker setup\n",
    "\n",
    "**When to Use:**\n",
    "- Development and testing environments\n",
    "- Low to moderate traffic applications\n",
    "- Scenarios without cache reuse patterns\n",
    "- When operational simplicity is prioritized over maximum performance\n",
    "\n",
    "**Trade-offs:**\n",
    "- Less flexibility in scaling individual components\n",
    "- No intelligent cache-aware routing\n",
    "- May not maximize resource utilization under high load\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- In the next notebook, you'll deploy a **disaggregated architecture** and compare the performance differences.\n",
    "\n",
    "\n",
    "**Continue to**: [vLLM Disagg Deployment](Dynamo_03_vLLM_disAgg_Deployment.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
